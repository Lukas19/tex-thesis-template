\secnumbersection{CONCLUSIONES}

    En la presente sección, se discuten las principales conclusiones que se obtuvieron una vez finalizado el trabajo.
    
\subsection{Proceso de minería y análisis de textos}
    Durante el desarrollo de esta memoria, se pudo apreciar cómo el uso de técnicas de análisis de textos, puede ser utilizada como una herramienta para la generación de directrices en el diseño de propuestas técnicas de trabajo. Para afirmar lo anterior, se procede a describir las conclusiones que cada uno de los experimentos realizados:
    
    \begin{itemize}
        \item \textbf{Análisis de keywords}: Los patrones que tuvieron un mayor grado de validación por parte del experto en el dominio en esta memoria, fueron los encontrados en este experimento. En general, realizar un análisis mediante \textit{keywords}, no es una tarea muy compleja de realizar. Esto ya que sólo son necesarios conocimientos sobre manejo de Strings y técnicas de conteo. De hecho, a nivel computacional no es costoso y como se comprobó en este trabajo, sus resultados suelen tener una buena aceptación.  Considerando todo lo anterior, se puede concluir que cuando se desea analizar un bajo número de documentos, una buena forma de encontrar patrones útiles es utilizando esta técnica de Análisis de Textos. 
        \item \textbf{Análisis de Dispersión Léxica}: Los patrones encontrados utilizando esta herramienta, también fueron validados por el experto del dominio, por lo que se considera como una herramienta valida a la hora de generar directrices para la generación de propuestas. Por otro lado, el principal desafío a la hora de generar este experimento, fue el desarrollo de la visualización. Esto ya que, las librerías existentes para analizar la dispersión léxica en documentos, sólo permiten realizar análisis para un conjunto de palabras en un único documento. Es por lo anterior que se debió generar desde cero la visualización, diseñada y presentada en la respectiva sección. Finalmente, gracias a la ayuda del experto, también se determinó que esta herramienta puede ser utilizada para entregar un primer diagnóstico a una propuesta en desarrollo.
        
        \item \textbf{Modelamiento de Tópicos}: Si bien el modelamiento de tópicos, es la técnica más compleja que se utilizó en este trabajo, sus resultados no fueron del todo generalizables en relación al objetivo principal de esta memoria. De hecho para el experto, el único patrón relevante encontrado por el uso de esta técnica, es la demostración de que el tema gestión de calidad es importante en los documentos de Hatch. Esto claramente ocurre, ya que el algoritmo utilizado requiere una mayor cantidad de documentos para generar conocimiento algo más útil. Por otro lado, si bien el uso de esta técnica no aportó al objetivo principal de esta memoria, si demostró ser una buena forma de agrupar documentos de forma rápida. Esto puede ser útil en Hatch, debido a que el sistema de gestión de documentos que actualmente utiliza la compañía, no posee ningún mecanismo para la extracción inteligente de los datos. Es decir, realizar extracciones por fecha, cliente o dominio del proyecto. Por ende, estos deben ser extraídos de forma manual y si se quisieran realizar agrupaciones de éstos, también deben hacerse de la misma forma.
        
        \item \textbf{Clustering de documentos}: El experimento de \textit{clustering} de documentos, utilizando los resultados de modelamiento de tópicos, en general tuvo resultados positivos siendo el más útil el patrón encontrado para el cliente ''Codelco División Chuquicamata``. La implementación de este experimento no fue de gran complicación y en general los resultados tienen un costo computacional bajo en términos de tiempos de ejecución. Sin embargo, la tarea más compleja a la hora de realizar un análisis mediante \textit{clustering}, es la caracterización de los clústers encontrados. Esto ya que como se quiere caracterizar un conjunto de documentos, además de analizar el texto en sí, se debe extraer la metada asociado a este (Nombre del proyecto y cliente en nuestro caso). Esto, con el fin de poder interpretar utilizando la mayor cantidad de información las agrupaciones encontradas por el algoritmo.
    \end{itemize}

\subsection{Resultados}
    En general, se lograron encontrar varios patrones en propuestas que habían sido declaradas ganadoras en sus respectivas licitaciones. Si bien, no todos estos patrones fueron validados por el experto en el dominio para ser agregados al producto final de esta memoria, si se valora la automatización y generalización de los experimentos para ser repetidos en un conjunto de datos más grande que el utilizado en este trabajo. Esto, con el fin de encontrar patrones con un mayor grado de confiabilidad y además más precisos.
    
    Por otro lado, la no aprobación de muchos de los patrones encontrados en este trabajo, se debe más que nada a la poca cantidad de datos analizados. Esto, ya que mucha de la información extraída tenía poca capacidad de ser generalizada, debido a que aparecía en un número ínfimo de propuestas.
    
    Finalmente, también se encontró como ciertas visualizaciones podían ser utilizadas como herramientas de diagnóstico durante el proceso de producción de una propuesta. Este fue el caso del gráfico de dispersión léxica, el cual puede entregar  un diagnóstico sobre si una propuesta está cumpliendo o no la estructura básica de una propuesta Hatch. Por ejemplo en nuestra guía, se nombró que toda propuesta en Hatch debe tener una sección de Gestión de calidad y otra de Seguridad, por lo que utilizando este gráfico se hace sencillo y rápido verificar el cumplimiento de esto.

\subsection{Dificultades a la hora de desarrollar el trabajo}
    Dentro del desarrollo de este trabajo, se presentaron algunos inconvenientes que se describen a continuación:
    \begin{itemize}
        \item \textbf{Extracción de datos}: Dentro de la compañía, si bien existe un sistema de gestión de documentos, éste no es más que un gestor de archivos. Es por ello, que la extracción de datos es sumamente lenta y se debe realizar de forma manual. Por otro lado, el acceso a estos documentos es sumamente restringido dentro de la compañía, por lo que acceder a dicho sistema requiere de ciertos privilegios que el autor de este documento no poseía. Es por lo anterior, que para obtener los documentos que fueron analizados en esta memoria se requirió ayuda de personal de la compañía. El problema aquí es que por ejemplo si se requiere obtener todos los documentos que fueron escritos para un cierto cliente $X$, se deben solicitar a alguna persona con los privilegios para acceder a esa información. Luego, dicha persona debe buscar uno a uno dentro del directorio los archivos que cumplan con la restricción solicitada. Es por lo anterior que el proceso de extracción de datos dentro de la compañía es sumamente lento. Para mejorar esto, es que se recomienda tener un sistema de bases de datos que permita extraer los documentos de una manera más rápida por las personas que tengan acceso a dichos documentos.
        \item \textbf{Pocas librerías de procesamiento de lenguaje natural en español}: Otra dificultad que se encontró al desarrollar este trabajo, es el poco desarrollo de liberías de procesamiento de lenguaje natural adaptadas al español. Por ejemplo, en el marco conceptual se explicaron conceptos como: Stemming, Lematización y Part of Speech. En general si bien hay librerías que realizan estas tareas básicas de procesamiento de lenguaje natural, la mayoría lo hace de forma bastante defectuosa para textos en español. Esto ocurre sobre todo a la hora de manejar conceptos técnicos, como es nuestro caso. Es por ello que cuando se diseñaron los experimentos, se decidió dejar de lado el uso de estas tareas en nuestro preprocesador de textos.
        \item \textbf{Sensibilidad con respecto al uso de datos}: Uno de los principales problemas que se tuvo durante el desarrollo de esta memoria, fue el rechazo y preocupación de ciertos directivos dentro de la compañía al percatarse que una persona externa a la organización estaba explorando las propuestas. Esto trajo problemas, ya que luego de esto, se disminuyó la cantidad de datos a analizar y por ende disminuyó la calidad de los resultados del trabajo. De esta experiencia, se puede ver que para ejecutar un proyecto de análisis y minería de textos, con datos sensibles como lo son las propuestas de una empresa, se debe conseguir el apoyo de todos los Stakeholders del proyecto antes de realizar cualquier análisis. Esto con el fin de asegurar el transcurso normal del proyecto.
        \item \textbf{Poco conocimiento en análisis de textos por parte del analista}: Otro problema que se debió resolver a medida que se desarrollaba esta memoria, fue el nulo conocimiento de análisis de textos por parte del autor antes de enfrentar el desafío impuesto por la memoria. Es por ello, que mucho del tiempo invertido en el desarrollo de este trabajo fue dedicado principalmente, al estudio sobre técnicas de análisis y minería de textos. 
        
    \end{itemize}

\subsection{Trabajo futuro}
    A partir del desarrollo de esta memoria, es claro que los patrones encontrados pudieron haber tenido una calidad superior. Es por ello que como trabajo futuro, se recomienda repetir los experimentos realizados en este trabajo pero esta vez, utilizando las recomendaciones del experto. Es decir, se deben repetir los experimentos utilizando una mayor cantidad de documentos y realizar análisis separados por clientes y volumen de venta de cada propuesta. 
    
    Por otro lado, como se mencionó anteriormente, otro problema detectado en esta memoria fue la forma en que se almacenaban las propuestas dentro de la compañía. Considerando el volumen de documentos almacenados por la compañía y la idea de realizar análisis de manera sostenida y continúa de aquí en mas, es que se hace insostenible seguir con el sistema actual, por ende se propone reemplazarlo por alguna base de datos más moderna. Esto debido a que la extracción de los datos utilizando el sistema actual se debe realizar de forma manual, haciendo el proceso lento y tedioso para la persona encargada de ejecutarlo. Por otro lado, dicha persona se puede transformar en un cuello de botella dentro del proyecto haciendo más larga la duración de este. 

\subsection{Conclusiones finales}
    En base a todos los experimentos efectuados sobre el conjunto de documentos analizados, es posible concluir que efectivamente este tipo de datos es apto para ser analizados utilizando técnicas de análisis de textos. De hecho, ciertos experimentos demostraron ser potenciales herramientas para realizar análisis de forma más eficiente a la estructura de una propuesta (análisis de dispersión léxica), mejorando el proceso de diagnóstico actual que se realiza en la compañía. Además realizando los experimentos con un número ínfimo de datos, se lograron encontrar algunos patrones, que no habían sido detectados, utilizando los métodos manuales que se efectúa hasta la fecha en la compañía. Por lo anterior se presume que escalar estos experimentos definitivamente entregarán mejores patrones y ayudarán a generar mejores directrices para el desarrollo de nuevas propuestas.



